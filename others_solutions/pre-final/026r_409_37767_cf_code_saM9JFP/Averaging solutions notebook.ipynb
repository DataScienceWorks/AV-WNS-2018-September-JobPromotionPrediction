{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import glob\n",
    "from operator import itemgetter\n",
    "\n",
    "from scipy.stats import hmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names=glob.glob('../utility/different runs with different cv/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_files = [f for f in file_names if '_train' in f]\n",
    "te_files = [f for f in file_names if '_test' in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_false_tr_files = [f for f in tr_files if '_train_base_lb_false' in f]\n",
    "cat_tr_files = [f for f in tr_files if 'train_base_cat' in f]\n",
    "lb_true_tr_files = [f for f in tr_files if f not in lb_false_tr_files + cat_tr_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_false_te_files = [f for f in te_files if '_base_lb_false' in f]\n",
    "cat_te_files = [f for f in te_files if 'base_cat' in f]\n",
    "lb_true_te_files = [f for f in te_files if f not in lb_false_te_files + cat_te_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 9, 25)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lb_false_te_files), len(cat_te_files), len(lb_true_te_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_false_tr_files = [pd.read_csv(f) for f in lb_false_tr_files]\n",
    "lb_false_te_files = [pd.read_csv(f) for f in lb_false_te_files]\n",
    "\n",
    "cat_tr_files = [pd.read_csv(f) for f in cat_tr_files]\n",
    "cat_te_files = [pd.read_csv(f) for f in cat_te_files]\n",
    "\n",
    "lb_true_tr_files = [pd.read_csv(f) for f in lb_true_tr_files]\n",
    "lb_true_te_files = [pd.read_csv(f) for f in lb_true_te_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_gm(tr_files, te_files, file_name, HM = True):\n",
    "    thresh_dict = {}\n",
    "    for j,f in enumerate(tr_files):\n",
    "        score_dict = {}\n",
    "        for i in np.arange(0, 1, 0.01):\n",
    "            score_dict[i] = f1_score(f['is_promoted'], (f['Prediction_prob'] > i).map({True:1, False:0}))\n",
    "        tmep = sorted(score_dict.items(), key = itemgetter(1), reverse = True)\n",
    "        thresh_dict[j] = tmep[0][0]\n",
    "        print(tmep[0][1])\n",
    "        \n",
    "    tr = pd.concat([f['Prediction_prob'] for f in tr_files], axis = 1)\n",
    "    if HM:\n",
    "        tr = tr.apply(lambda x: hmean(x), axis = 1)\n",
    "    else:\n",
    "        tr = tr.apply(lambda x: np.prod(x)**(1/len(x)), axis = 1)\n",
    "    \n",
    "    oof_preds = tr_files[0][['employee_id', 'is_promoted']]\n",
    "    oof_preds['Prediction_prob'] = tr\n",
    "    oof_preds.to_csv('../utility/OOF_Proba_'+file_name, index = False)\n",
    "    score_dict = {}\n",
    "    for i in np.arange(0, 1, 0.01):\n",
    "        score_dict[i] = f1_score(tr_files[0]['is_promoted'], (tr >= i).map({True:1, False:0}))\n",
    "    sorted_dict = sorted(score_dict.items(), key = itemgetter(1), reverse = True) \n",
    "    optimum_th = sorted_dict[0][0]\n",
    "    cv_score = sorted_dict[0][1]\n",
    "    print(f'cv score is {cv_score}')\n",
    "    \n",
    "    sub = te_files[0][['employee_id']]\n",
    "    s = pd.concat([f['is_promoted_proba'] for f in te_files], axis = 1)\n",
    "    if HM:\n",
    "        sub['is_promoted'] = (s.apply(lambda x: hmean(x), axis = 1) > optimum_th).map({True:1, False: 0})\n",
    "    else:\n",
    "        sub['is_promoted'] = (s.apply(lambda x: np.prod(x)**(1/len(x)), axis = 1) > optimum_th).map({True:1, False: 0})\n",
    "    sub.to_csv('../output/'+file_name, index = False)\n",
    "    if HM:\n",
    "        sub['is_promoted_proba'] = s.apply(lambda x: hmean(x), axis = 1)\n",
    "    else:\n",
    "        sub['is_promoted_proba'] = s.apply(lambda x: np.prod(x)**(1/len(x)), axis = 1)\n",
    "    sub[['employee_id', 'is_promoted_proba']].to_csv('../utility/Proba_'+file_name, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5196812364163246\n",
      "0.5169593779326987\n",
      "0.5178621659634317\n",
      "0.5223300970873787\n",
      "0.5207100591715976\n",
      "0.515645894624231\n",
      "0.5189787923814669\n",
      "0.5188692382676506\n",
      "0.5191310441485635\n",
      "0.5201845444059977\n",
      "0.5199676200755532\n",
      "0.5173340880147164\n",
      "0.5142528073711488\n",
      "0.5159208865198485\n",
      "0.5194733194733194\n",
      "0.5184094256259205\n",
      "0.5169360505973295\n",
      "0.5187552097804946\n",
      "0.5153132902500702\n",
      "0.5179148517633835\n",
      "0.5201805992611849\n",
      "0.5193370165745855\n",
      "0.5171270718232044\n",
      "0.5163323782234956\n",
      "0.5189247024207571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abk0005/Competitions/env/lib/python3.6/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv score is 0.5207209577687146\n"
     ]
    }
   ],
   "source": [
    "getting_gm(lb_true_tr_files, lb_true_te_files, 'lb_true_lgb_5x5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5267552182163188\n",
      "0.5248893063196028\n",
      "0.5249095463401058\n",
      "0.5254967347505906\n",
      "0.5267489711934156\n",
      "0.5255674049767568\n",
      "0.5268169724128274\n",
      "0.5302972802024035\n",
      "0.5261945883707542\n",
      "0.5256285595221558\n",
      "0.5277445109780439\n",
      "0.5281104692123432\n",
      "0.5307092854312508\n",
      "0.5259496312786976\n",
      "0.5243851604835348\n",
      "0.5271112289539971\n",
      "0.526211671612265\n",
      "0.5293889678336131\n",
      "0.5261368465788355\n",
      "0.5254955570745045\n",
      "0.5296453512255237\n",
      "0.5254189944134079\n",
      "0.5269768769626035\n",
      "0.527094258437542\n",
      "0.5301650165016502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abk0005/Competitions/env/lib/python3.6/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv score is 0.5299167877427025\n"
     ]
    }
   ],
   "source": [
    "getting_gm(lb_false_tr_files, lb_false_te_files, 'lb_false_lgb_5x5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5231255386383223\n",
      "0.5224849895846096\n",
      "0.5232753785754347\n",
      "0.5313152400835074\n",
      "0.5266666666666667\n",
      "0.523783185840708\n",
      "0.5308528756714267\n",
      "0.5232497387669801\n",
      "0.5291902071563088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abk0005/Competitions/env/lib/python3.6/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv score is 0.5273545203889131\n"
     ]
    }
   ],
   "source": [
    "getting_gm(cat_tr_files, cat_te_files, 'cat_3x3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_model1 = pd.read_csv('../utility/OOF_Proba_lb_false_lgb_5x5.csv')\n",
    "oof_model2 = pd.read_csv('../utility/OOF_Proba_lb_true_lgb_5x5.csv')\n",
    "oof_model3 = pd.read_csv('../utility/OOF_Proba_cat_3x3.csv')\n",
    "\n",
    "test_model1 = pd.read_csv('../utility/Proba_lb_true_lgb_5x5.csv')\n",
    "test_model2 = pd.read_csv('../utility/Proba_lb_false_lgb_5x5.csv')\n",
    "test_model3 = pd.read_csv('../utility/Proba_cat_3x3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54802,)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_preds = pd.concat([oof_model1['Prediction_prob'], oof_model2['Prediction_prob'], oof_model3['Prediction_prob']],\n",
    "                      axis = 1)\n",
    "tr = oof_preds.apply(lambda x: hmean(x), axis = 1)\n",
    "tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23490,)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = pd.concat([test_model1['is_promoted_proba'], test_model2['is_promoted_proba'], test_model3['is_promoted_proba']],\n",
    "                      axis = 1)\n",
    "preds = preds.apply(lambda x: hmean(x), axis = 1)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict = {}\n",
    "for i in np.arange(0, 1, 0.01):\n",
    "    score_dict[i] = f1_score(oof_model1['is_promoted'], (tr >= i).map({True:1, False:0}))\n",
    "sorted_dict = sorted(score_dict.items(), key = itemgetter(1), reverse = True) \n",
    "optimum_th = sorted_dict[0][0]\n",
    "cv_score = sorted_dict[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.27, 0.5297961317302666)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimum_th, cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.054235845040442744"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = test_model1[['employee_id']]\n",
    "sub['is_promoted'] = (preds > 0.27).map({True:1, False: 0})\n",
    "sub['is_promoted'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('../output/Final_submission_wo_etc.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
