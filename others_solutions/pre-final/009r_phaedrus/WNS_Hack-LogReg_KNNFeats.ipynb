{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from fastai.structured import *\n",
    "from fastai.column_data import *\n",
    "np.set_printoptions(threshold=50, edgeitems=20)\n",
    "pd.set_option('display.max_columns', 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score ,roc_curve,auc\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "seed =45\n",
    "% matplotlib inline\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### read data\n",
    "PATH='F:/AV/WNS'\n",
    "train_csv = 'train_LZdllcl.csv'\n",
    "test_csv = 'test_2umaH9m.csv'\n",
    "submit_csv = 'sample_submission_M0L0uXE.csv'\n",
    "\n",
    "### read train, test and submission files\n",
    "train = pd.read_csv(f'{PATH}/{train_csv}')\n",
    "test = pd.read_csv(f'{PATH}/{test_csv}')\n",
    "submission = pd.read_csv(f'{PATH}/{submit_csv}')\n",
    "\n",
    "print(\"Shape of {}:{} {}:{} {}:{}\".format('train',train.shape,'test',test.shape,'submission',submission.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### inspect data\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### class balance\n",
    "\n",
    "train['is_promoted'].value_counts()\n",
    "\n",
    "## so approx 10% of past employees have been promoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### lets check if there is any repeat in employees\n",
    "\n",
    "len(train['employee_id'].unique()) == train.shape[0]\n",
    "\n",
    "### so all IDs are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## null values\n",
    "\n",
    "null_columns=train.columns[train.isnull().any()]\n",
    "train[null_columns].isnull().sum()\n",
    "\n",
    "## so 2 columns have null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### inspect null value columns\n",
    "a = train[(train.education.isnull())]\n",
    "_ = train[(train.education.isnull() | train.previous_year_rating.isnull())]\n",
    "\n",
    "print(a.shape,_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(a.index).intersection(set(_.index)) == set(a.index)\n",
    "\n",
    "### so everywhere where education is not present prev year rating is also not present, but vice-versa is not true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check avg % of people promoted with NA in previous ye rating vs without NA\n",
    "\n",
    "print(np.mean(_['is_promoted']),np.mean(train[~train.education.isnull()]['is_promoted']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check avg % of people promoted with NA in education vs without NA\n",
    "\n",
    "print(np.mean(a['is_promoted']),np.mean(train[~train.education.isnull()]['is_promoted']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For education we will use unknown for all missing values and 9999 for prev year training\n",
    "\n",
    "train['education'] = train.education.fillna('unknown')\n",
    "train['previous_year_rating'] = train.previous_year_rating.fillna(9999)\n",
    "\n",
    "test['education'] = test.education.fillna('unknown')\n",
    "test['previous_year_rating'] = test.previous_year_rating.fillna(9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### merge dataframes for ease of processing\n",
    "Y = train['is_promoted'].values\n",
    "train.drop('is_promoted',inplace=True,axis=1)\n",
    "train['train'] = 'train'\n",
    "test['train'] = 'test'\n",
    "merged = pd.concat([train,test])\n",
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [i for i in merged.columns if merged[i].dtypes == 'object']+['KPIs_met >80%','awards_won?']\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols.remove('train')\n",
    "print(cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(df,cat_cols):\n",
    "    one_hot_encoded_training_predictors = pd.get_dummies(df[cat_cols])\n",
    "    df.drop(cat_cols,inplace=True,axis=1)\n",
    "    _ = pd.concat([df,one_hot_encoded_training_predictors],1)\n",
    "    new_tr, new_tst = _[_['train']=='train'],_[_['train']=='test']\n",
    "    new_tr.drop('train',inplace=True,axis=1)\n",
    "    new_tst.drop('train',inplace=True,axis=1)\n",
    "    return new_tr, new_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_OHE,test_OHE = pre_process(merged,cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### nearest neightbour features\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tqdm import tqdm\n",
    "\n",
    "def make_unsupervised_knn_HC(N_NEIGHBORS):\n",
    "    \"\"\"создает колонки с расстоянием от исходной точки до k-го соседа\"\"\"\n",
    "    #df = df.drop(\"TARGET\",1)\n",
    "    model = NearestNeighbors(n_neighbors=N_NEIGHBORS, algorithm = 'ball_tree',n_jobs = -1) # ball tree works faster, for better results use 'auto'\n",
    "    model.fit(train_OHE)\n",
    "    k_distances, indices = model.kneighbors(train_OHE)\n",
    "    k_distances_test, indices_test = model.kneighbors(test_OHE)\n",
    "\n",
    "    for i in tqdm(range(1, N_NEIGHBORS)):\n",
    "        print(i)\n",
    "        train_OHE[\"dist_{}_neigh\".format(i)] = k_distances[:, i]\n",
    "        test_OHE[\"dist_{}_neigh\".format(i)] = k_distances_test[:, i]\n",
    "#    df.to_csv(\"knn_dataset.csv\")\n",
    "    return train_OHE,test_OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_OHE,test_OHE = make_unsupervised_knn_HC(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_OHE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define X, Y\n",
    "X_cols = [i for i in train_OHE.columns]\n",
    "X_cols.remove('employee_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid Search\n",
    "logreg = LogisticRegression(class_weight='balanced')\n",
    "param = {'C':[0.001,0.01,0.1,1,10,100]}\n",
    "clf = GridSearchCV(logreg,param,scoring='f1',refit=True,cv=10,verbose = 1)\n",
    "clf.fit(train_OHE[X_cols],Y)\n",
    "print('Best F1: {:.4}, with best C: {}'.format(clf.best_score_, clf.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_c = clf.best_params_['C']\n",
    "print(best_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_OHE = train_OHE.reset_index()\n",
    "# test_OHE = test_OHE.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutoff_predict_own(pred,cutoff):\n",
    "    return ((pred>cutoff).astype(int))\n",
    "\n",
    "def cutoff_predict(clf,X,cutoff):\n",
    "    return ((clf.predict_proba(X)[:,1]>cutoff).astype(int))\n",
    "\n",
    "def custom_f1(y,pred,cutoff):\n",
    "    ypred = cutoff_predict_own(pred,cutoff)\n",
    "    scr = sklearn.metrics.f1_score(y,ypred)\n",
    "    return scr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=5,shuffle=True,random_state=seed)\n",
    "pred_test_full =0\n",
    "cv_score =[]\n",
    "i=1\n",
    "X = train_OHE[X_cols]\n",
    "y = Y\n",
    "x_test = test_OHE[X_cols]\n",
    "custom = []\n",
    "holdy = []\n",
    "holdout = []\n",
    "for train_index,test_index in kf.split(X,y):\n",
    "    print('{} of KFold {}'.format(i,kf.n_splits))\n",
    "    xtr,xvl = X.iloc[train_index],X.iloc[test_index]\n",
    "    ytr,yvl = y[train_index],y[test_index]\n",
    "    \n",
    "    #model\n",
    "    lr = LogisticRegression(C=int(best_c))\n",
    "    lr.fit(xtr,ytr)\n",
    "    _ = lr.predict(xvl)\n",
    "    temp1 = lr.predict_proba(xvl)[:,1]\n",
    "    holdout.append(list(temp1))\n",
    "    holdy.append(list(yvl))\n",
    "\n",
    "    ##score\n",
    "    score = sklearn.metrics.f1_score(yvl,_)\n",
    "    print('F1:',score)\n",
    "    cv_score.append(score)    \n",
    "    pred_test = lr.predict_proba(x_test)[:,1]\n",
    "    pred_test_full +=pred_test\n",
    "#    print(len(holdout))\n",
    "\n",
    " \n",
    "    if i == 5:\n",
    "        holdout = [item for sublist in holdout for item in sublist]\n",
    "        holdy = [item for sublist in holdy for item in sublist]\n",
    "        print(\"Running finetune for threshold\")\n",
    "        for cutoff in np.arange(0.2,0.5,0.05):\n",
    "            custom_scr = custom_f1(holdy,holdout, cutoff)\n",
    "            custom.append({'fld':i,'cutoff':np.round(cutoff,2),'custom':np.round(custom_scr,4)})\n",
    "            \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_full /= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_submit(predictions,modelname='LogReg_knn_v0'):\n",
    "    submission['is_promoted'] = predictions\n",
    "    _ = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "    Fname = 'F:/AV/WNS/submission/'+str(modelname)+'_'+str(_)+'.csv'\n",
    "    print(Fname)\n",
    "    submission.to_csv(Fname,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submit(pred_test_full,modelname='LogReg_knn_v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
