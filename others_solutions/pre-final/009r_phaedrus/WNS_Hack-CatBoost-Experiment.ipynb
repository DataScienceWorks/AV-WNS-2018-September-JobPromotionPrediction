{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from fastai.structured import *\n",
    "from fastai.column_data import *\n",
    "np.set_printoptions(threshold=50, edgeitems=20)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "from lightgbm import LGBMClassifier\n",
    "from datetime import datetime\n",
    "from catboost import CatBoostClassifier, Pool, cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score ,roc_curve,auc\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold,KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "seed = 45\n",
    "#seed =145\n",
    "% matplotlib inline\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### read data with some categorty name changes to raw files\n",
    "PATH='F:/AV/WNS'\n",
    "train_csv = 'train_catboost.csv'\n",
    "test_csv = 'test_catboost.csv'\n",
    "submit_csv = 'sample_submission_M0L0uXE.csv'\n",
    "\n",
    "### read train, test and submission files\n",
    "train = pd.read_csv(f'{PATH}/{train_csv}')\n",
    "test = pd.read_csv(f'{PATH}/{test_csv}')\n",
    "submission = pd.read_csv(f'{PATH}/{submit_csv}')\n",
    "\n",
    "print(\"Shape of {}:{} {}:{} {}:{}\".format('train',train.shape,'test',test.shape,'submission',submission.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### inspect data\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### class balance\n",
    "\n",
    "train['is_promoted'].value_counts()\n",
    "\n",
    "## so approx 10% of past employees have been promoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### lets check if there is any repeat in employees\n",
    "\n",
    "len(train['employee_id'].unique()) == train.shape[0]\n",
    "\n",
    "### so all IDs are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## null values\n",
    "\n",
    "null_columns=train.columns[train.isnull().any()]\n",
    "train[null_columns].isnull().sum()\n",
    "\n",
    "## so 2 columns have null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### inspect null value columns\n",
    "a = train[(train.education.isnull())]\n",
    "_ = train[(train.education.isnull() | train.previous_year_rating.isnull())]\n",
    "\n",
    "print(a.shape,_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(a.index).intersection(set(_.index)) == set(a.index)\n",
    "\n",
    "### so everywhere where education is not present prev year rating is also not present, but vice-versa is not true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check avg % of people promoted with NA in previous ye rating vs without NA\n",
    "\n",
    "print(np.mean(_['is_promoted']),np.mean(train[~train.education.isnull()]['is_promoted']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check avg % of people promoted with NA in education vs without NA\n",
    "\n",
    "print(np.mean(a['is_promoted']),np.mean(train[~train.education.isnull()]['is_promoted']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For education we will use unknown for all missing values and 9999 for prev year training\n",
    "\n",
    "train['education'] = train.education.fillna('unknown')\n",
    "train['previous_year_rating'] = train.previous_year_rating.fillna(9999)\n",
    "\n",
    "test['education'] = test.education.fillna('unknown')\n",
    "test['previous_year_rating'] = test.previous_year_rating.fillna(9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### merge dataframes for ease of processing\n",
    "Y = train['is_promoted'].values\n",
    "train.drop('is_promoted',inplace=True,axis=1)\n",
    "train['train'] = 'train'\n",
    "test['train'] = 'test'\n",
    "merged = pd.concat([train,test])\n",
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [i for i in merged.columns if merged[i].dtypes == 'object']+['KPIs_met >80%','awards_won?','previous_year_rating']\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols.remove('train')\n",
    "print(cat_cols)\n",
    "#cat_idx = [merged.columns.get_loc(c) for c in merged.columns if c in cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tr_cols = [i for i in merged.columns]\n",
    "tr_cols.remove('employee_id')\n",
    "tr  = merged[merged['train']=='train']\n",
    "te = merged[merged['train']=='test']\n",
    "tr.drop('train',axis=1,inplace=True)\n",
    "te.drop('train',axis=1,inplace=True)\n",
    "\n",
    "print(tr_cols)\n",
    "\n",
    "### ###\n",
    "tr_cols.remove('train')\n",
    "X_train = tr[tr_cols]\n",
    "Y_train = Y\n",
    "X_test = te[tr_cols]\n",
    "\n",
    "#cat_idx = [X_train.columns.get_loc(c) for c in X_train.columns if c in cat_cols]\n",
    "cat_idx = [X_train.columns.get_loc(c) for c in cat_cols]\n",
    "\n",
    "train_pool = Pool(X_train, Y_train, cat_features=cat_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.iloc[:,cat_idx].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pre_process(df,cat_cols):\n",
    "#     one_hot_encoded_training_predictors = pd.get_dummies(df[cat_cols])\n",
    "#     df.drop(cat_cols,inplace=True,axis=1)\n",
    "#     _ = pd.concat([df,one_hot_encoded_training_predictors],1)\n",
    "#     new_tr, new_tst = _[_['train']=='train'],_[_['train']=='test']\n",
    "#     new_tr.drop('train',inplace=True,axis=1)\n",
    "#     new_tst.drop('train',inplace=True,axis=1)\n",
    "#     return new_tr, new_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_OHE,test_OHE = pre_process(merged,cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_lcatboost(train_df,test_df, target,num_folds, stratified = False, debug= False,modelname=\"catboost\"):\n",
    "    # Divide in training/validation and test data\n",
    "    train_df = train_df\n",
    "    test_df = test_df\n",
    "    print(\"Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
    "    gc.collect()\n",
    "\n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=seed)\n",
    "    else:\n",
    "        folds = KFold(n_splits= num_folds, shuffle=True, random_state=seed)\n",
    "    # Create arrays and dataframes to store results\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feats = [f for f in train_df.columns if f not in ['employee_id','index']]\n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], target)):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], target[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], target[valid_idx]\n",
    "\n",
    "        # LightGBM parameters found by Bayesian optimization\n",
    "        clf = CatBoostClassifier(\n",
    "            #nthread=4,\n",
    "            iterations=5000,\n",
    "            bagging_temperature = 1.5,\n",
    "            learning_rate=0.01,\n",
    "            l2_leaf_reg = 1.25,\n",
    "            depth=12,\n",
    "            loss_function='Logloss',\n",
    "            eval_metric='F1',\n",
    "            silent=False)\n",
    "        #print(train_x.head())\n",
    "        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "            verbose= 100, early_stopping_rounds= 200,cat_features=cat_idx,use_best_model=True)\n",
    "\n",
    "        oof_preds[valid_idx] = clf.predict_proba(valid_x)[:, 1]\n",
    "        sub_preds += clf.predict_proba(test_df[feats])[:, 1] / folds.n_splits\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        print('Fold %2d F-score : %.6f' % (n_fold + 1, sklearn.metrics.f1_score(valid_y, (oof_preds[valid_idx]>0.3).astype(int))))\n",
    "        del clf, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    print('Full f1 score %.6f' % sklearn.metrics.f1_score(target, (oof_preds>0.3).astype(int)))\n",
    "    \n",
    "    # Write submission file and plot feature importance\n",
    "    if not debug:\n",
    "        _ = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "        Fname = 'F:/AV/WNS/submission/'+str(modelname)+'_'+str(_)+'.csv'\n",
    "        submission['is_promoted'] = sub_preds\n",
    "        submission[['employee_id', 'is_promoted']].to_csv(Fname, index= False)\n",
    "        oof = pd.DataFrame(oof_preds)\n",
    "        score = sklearn.metrics.f1_score(target, (oof_preds>0.3).astype(int))\n",
    "        oof.columns = [modelname+'_'+str(round(score,4))]\n",
    "        OOF_Fname = 'F:/AV/WNS/oof/'+str(modelname)+'_'+str(_)+'.csv'\n",
    "        oof.to_csv(OOF_Fname,index=False)\n",
    "    #display_importances(feature_importance_df)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "oof = kfold_lcatboost(X_train,X_test, Y,num_folds=10, stratified = True, debug= False,modelname=\"catboost_10fld_depth12_L2_1.25_temp_1.25\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
