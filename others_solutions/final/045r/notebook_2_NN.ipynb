{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from keras.layers import Input, BatchNormalization, GaussianDropout, concatenate, Dense, PReLU, Dropout, ReLU\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.initializers import he_normal, glorot_normal\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "train & test ( Sat Sep 15 22:31:56 2018 )\n",
      "--------------------\n",
      "['department', 'region', 'education', 'gender', 'recruitment_channel', 'no_of_trainings', 'previous_year_rating', 'KPIs_met >80%', 'awards_won?', 'length_of_service', 'age', 'KPI_award', 'region_channel', 'education_region']\n",
      "     DF shape: (78292, 443)\n",
      "After removing empty features there are 439 features\n",
      "After removing features with the same distribution on 0 and 1 classes there are 216 features\n",
      "After removing features with not the same distribution on train and test datasets there are 216 features\n",
      "After removing features which are having information value less than 0.02 and greater than 0.8s 211 features\n"
     ]
    }
   ],
   "source": [
    "df =aggregate(lb_encode=False)\n",
    "df = clean_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['_'.join(c.split()) for c in df.columns]\n",
    "df.columns = [c.replace(\"'\", '_') for c in df.columns]\n",
    "df.columns = [c.replace(\"&\", '_') for c in df.columns]\n",
    "df.columns = [c.replace(\">\", '_') for c in df.columns]\n",
    "df.columns = [c.replace(\"%\", '_') for c in df.columns]\n",
    "df.columns = [c.replace(\"?\", '_') for c in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [c for c in df.columns if c not in ['is_promoted', 'employee_id']]\n",
    "corrupt_df = df_inputSwapNoise(df[feats], p = 0.2)\n",
    "corrupt_df = corrupt_df.apply(rank_gauss, axis = 1)\n",
    "df_feats=df[feats].apply(rank_gauss, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_keras_data(df):\n",
    "    X = []\n",
    "    for col in df.columns:\n",
    "        if col not in ['employee_id', 'is_promoted']:\n",
    "            X.append(np.array(df[col]))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DAE(inp_layers_name):\n",
    "    inp_layers = []\n",
    "    for i in inp_layers_name:\n",
    "        inp_layers.append(Input(shape = [1]))\n",
    "    \n",
    "    x = concatenate(inp_layers)\n",
    "    x = Dense(units=1500, kernel_initializer='glorot_normal')(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    enc1 = Dense(units=1500, kernel_initializer='glorot_normal')(x)\n",
    "    x = ReLU()(enc1)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    enc2 = Dense(units=1500, kernel_initializer='glorot_normal')(x)\n",
    "    x = ReLU()(enc2)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    dec = Dense(len(inp_layers_name))(x)\n",
    "    \n",
    "    model = Model(inp_layers, dec)\n",
    "    enc = Model(inp_layers, concatenate([enc1, enc2]))\n",
    "    optim = SGD(lr=0.0003, decay=0.995, momentum=0.99, clipnorm = 1)\n",
    "    #optim = Adam(lr=0.0003, decay=0.995, clipvalue = 0.1)\n",
    "    model.compile(optimizer = optim, loss='mean_squared_error')\n",
    "    \n",
    "    return model, enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = prep_keras_data(corrupt_df)\n",
    "\n",
    "inp_names = [c for c in corrupt_df.columns if c not in ['employee_id', 'is_promoted']]\n",
    "\n",
    "output = df_feats[inp_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "78292/78292 [==============================] - 67s 861us/step - loss: 0.5799\n",
      "Epoch 2/200\n",
      "78292/78292 [==============================] - 64s 821us/step - loss: 0.5270\n",
      "Epoch 3/200\n",
      "78292/78292 [==============================] - 65s 826us/step - loss: 0.5166\n",
      "Epoch 4/200\n",
      "78292/78292 [==============================] - 63s 805us/step - loss: 0.51061\n",
      "Epoch 5/200\n",
      "78292/78292 [==============================] - 71s 901us/step - loss: 0.5061\n",
      "Epoch 6/200\n",
      "78292/78292 [==============================] - 64s 815us/step - loss: 0.5027\n",
      "Epoch 7/200\n",
      "78292/78292 [==============================] - 62s 797us/step - loss: 0.4999\n",
      "Epoch 8/200\n",
      "78292/78292 [==============================] - 66s 849us/step - loss: 0.4977\n",
      "Epoch 9/200\n",
      "78292/78292 [==============================] - 64s 819us/step - loss: 0.4958\n",
      "Epoch 10/200\n",
      "78292/78292 [==============================] - 63s 809us/step - loss: 0.4938\n",
      "Epoch 11/200\n",
      "78292/78292 [==============================] - 70s 898us/step - loss: 0.4925\n",
      "Epoch 12/200\n",
      "78292/78292 [==============================] - 64s 821us/step - loss: 0.4910\n",
      "Epoch 13/200\n",
      "78292/78292 [==============================] - 58s 740us/step - loss: 0.4899\n",
      "Epoch 14/200\n",
      "78292/78292 [==============================] - 54s 685us/step - loss: 0.4886\n",
      "Epoch 15/200\n",
      "78292/78292 [==============================] - 53s 681us/step - loss: 0.4875\n",
      "Epoch 16/200\n",
      "78292/78292 [==============================] - 61s 777us/step - loss: 0.4867\n",
      "Epoch 17/200\n",
      "78292/78292 [==============================] - 53s 677us/step - loss: 0.4859\n",
      "Epoch 18/200\n",
      "78292/78292 [==============================] - 53s 677us/step - loss: 0.4851\n",
      "Epoch 19/200\n",
      "78292/78292 [==============================] - 53s 677us/step - loss: 0.4842\n",
      "Epoch 20/200\n",
      "78292/78292 [==============================] - 53s 680us/step - loss: 0.4833\n",
      "Epoch 21/200\n",
      "78292/78292 [==============================] - 53s 680us/step - loss: 0.4829\n",
      "Epoch 22/200\n",
      "78292/78292 [==============================] - 53s 677us/step - loss: 0.4821\n",
      "Epoch 23/200\n",
      "78292/78292 [==============================] - 53s 678us/step - loss: 0.4815\n",
      "Epoch 24/200\n",
      "78292/78292 [==============================] - 59s 748us/step - loss: 0.4810\n",
      "Epoch 25/200\n",
      "78292/78292 [==============================] - 60s 765us/step - loss: 0.4803\n",
      "Epoch 26/200\n",
      "78292/78292 [==============================] - 57s 730us/step - loss: 0.4800\n",
      "Epoch 27/200\n",
      "78292/78292 [==============================] - 58s 747us/step - loss: 0.4793\n",
      "Epoch 28/200\n",
      "78292/78292 [==============================] - 56s 719us/step - loss: 0.4789\n",
      "Epoch 29/200\n",
      "78292/78292 [==============================] - 62s 797us/step - loss: 0.4785\n",
      "Epoch 30/200\n",
      "78292/78292 [==============================] - 63s 802us/step - loss: 0.4781\n",
      "Epoch 31/200\n",
      "78292/78292 [==============================] - 70s 888us/step - loss: 0.4778\n",
      "Epoch 32/200\n",
      "78292/78292 [==============================] - 64s 818us/step - loss: 0.4773\n",
      "Epoch 33/200\n",
      "78292/78292 [==============================] - 70s 895us/step - loss: 0.4770\n",
      "Epoch 34/200\n",
      "78292/78292 [==============================] - 62s 798us/step - loss: 0.4766\n",
      "Epoch 35/200\n",
      "78292/78292 [==============================] - 54s 696us/step - loss: 0.4764\n",
      "Epoch 36/200\n",
      "78292/78292 [==============================] - 76s 976us/step - loss: 0.4760\n",
      "Epoch 37/200\n",
      "78292/78292 [==============================] - 148s 2ms/step - loss: 0.4757\n",
      "Epoch 38/200\n",
      "78292/78292 [==============================] - 145s 2ms/step - loss: 0.4754\n",
      "Epoch 39/200\n",
      "78292/78292 [==============================] - 142s 2ms/step - loss: 0.4750\n",
      "Epoch 40/200\n",
      "78292/78292 [==============================] - 141s 2ms/step - loss: 0.4749\n",
      "Epoch 41/200\n",
      "78292/78292 [==============================] - 52s 670us/step - loss: 0.4745\n",
      "Epoch 42/200\n",
      "78292/78292 [==============================] - 52s 662us/step - loss: 0.4743\n",
      "Epoch 43/200\n",
      "78292/78292 [==============================] - 52s 665us/step - loss: 0.4741\n",
      "Epoch 44/200\n",
      "78292/78292 [==============================] - 52s 658us/step - loss: 0.4738\n",
      "Epoch 45/200\n",
      "78292/78292 [==============================] - 52s 670us/step - loss: 0.4737\n",
      "Epoch 46/200\n",
      "78292/78292 [==============================] - 51s 657us/step - loss: 0.4731\n",
      "Epoch 47/200\n",
      "78292/78292 [==============================] - 52s 670us/step - loss: 0.4731\n",
      "Epoch 48/200\n",
      "78292/78292 [==============================] - 52s 666us/step - loss: 0.4727\n",
      "Epoch 49/200\n",
      "78292/78292 [==============================] - 52s 667us/step - loss: 0.4728\n",
      "Epoch 50/200\n",
      "78292/78292 [==============================] - 52s 670us/step - loss: 0.4724\n",
      "Epoch 51/200\n",
      "78292/78292 [==============================] - 52s 663us/step - loss: 0.4724\n",
      "Epoch 52/200\n",
      "78292/78292 [==============================] - 53s 673us/step - loss: 0.4721\n",
      "Epoch 53/200\n",
      "78292/78292 [==============================] - 51s 650us/step - loss: 0.4719\n",
      "Epoch 54/200\n",
      "78292/78292 [==============================] - 52s 664us/step - loss: 0.4717\n",
      "Epoch 55/200\n",
      "78292/78292 [==============================] - 52s 658us/step - loss: 0.4715\n",
      "Epoch 56/200\n",
      "78292/78292 [==============================] - 52s 669us/step - loss: 0.4715\n",
      "Epoch 57/200\n",
      "78292/78292 [==============================] - 52s 662us/step - loss: 0.4711\n",
      "Epoch 58/200\n",
      "78292/78292 [==============================] - 52s 659us/step - loss: 0.4710\n",
      "Epoch 59/200\n",
      "78292/78292 [==============================] - 52s 663us/step - loss: 0.4710\n",
      "Epoch 60/200\n",
      "78292/78292 [==============================] - 51s 655us/step - loss: 0.4710\n",
      "Epoch 61/200\n",
      "78292/78292 [==============================] - 52s 670us/step - loss: 0.4708\n",
      "Epoch 62/200\n",
      "78292/78292 [==============================] - 52s 662us/step - loss: 0.4704\n",
      "Epoch 63/200\n",
      "78292/78292 [==============================] - 54s 685us/step - loss: 0.4702\n",
      "Epoch 64/200\n",
      "78292/78292 [==============================] - 53s 674us/step - loss: 0.4702\n",
      "Epoch 65/200\n",
      "78292/78292 [==============================] - 53s 678us/step - loss: 0.4702\n",
      "Epoch 66/200\n",
      "78292/78292 [==============================] - 54s 687us/step - loss: 0.4701\n",
      "Epoch 67/200\n",
      "78292/78292 [==============================] - 54s 688us/step - loss: 0.4698\n",
      "Epoch 68/200\n",
      "78292/78292 [==============================] - 53s 683us/step - loss: 0.4699\n",
      "Epoch 69/200\n",
      "78292/78292 [==============================] - 52s 667us/step - loss: 0.4696\n",
      "Epoch 70/200\n",
      "78292/78292 [==============================] - 53s 681us/step - loss: 0.4696\n",
      "Epoch 71/200\n",
      "78292/78292 [==============================] - 55s 699us/step - loss: 0.4695\n",
      "Epoch 72/200\n",
      "78292/78292 [==============================] - 55s 704us/step - loss: 0.4693\n",
      "Epoch 73/200\n",
      "78292/78292 [==============================] - 53s 676us/step - loss: 0.4691\n",
      "Epoch 74/200\n",
      "78292/78292 [==============================] - 53s 678us/step - loss: 0.4692\n",
      "Epoch 75/200\n",
      "78292/78292 [==============================] - 54s 685us/step - loss: 0.4690\n",
      "Epoch 76/200\n",
      "78292/78292 [==============================] - 53s 674us/step - loss: 0.4688\n",
      "Epoch 77/200\n",
      "78292/78292 [==============================] - 54s 695us/step - loss: 0.4690\n",
      "Epoch 78/200\n",
      "78292/78292 [==============================] - 53s 680us/step - loss: 0.4688\n",
      "Epoch 79/200\n",
      "78292/78292 [==============================] - 54s 688us/step - loss: 0.4687\n",
      "Epoch 80/200\n",
      "78292/78292 [==============================] - 52s 670us/step - loss: 0.4686\n",
      "Epoch 81/200\n",
      "78292/78292 [==============================] - 53s 681us/step - loss: 0.4686\n",
      "Epoch 82/200\n",
      "78292/78292 [==============================] - 53s 675us/step - loss: 0.4683\n",
      "Epoch 83/200\n",
      "78292/78292 [==============================] - 53s 677us/step - loss: 0.4684\n",
      "Epoch 84/200\n",
      "78292/78292 [==============================] - 53s 678us/step - loss: 0.4681\n",
      "Epoch 85/200\n",
      "78292/78292 [==============================] - 53s 677us/step - loss: 0.4680\n",
      "Epoch 86/200\n",
      "78292/78292 [==============================] - 53s 677us/step - loss: 0.4680\n",
      "Epoch 87/200\n",
      "78292/78292 [==============================] - 53s 671us/step - loss: 0.4681\n",
      "Epoch 88/200\n",
      "78292/78292 [==============================] - 53s 681us/step - loss: 0.4679\n",
      "Epoch 89/200\n",
      "78292/78292 [==============================] - 52s 670us/step - loss: 0.4679\n",
      "Epoch 90/200\n",
      "78292/78292 [==============================] - 53s 677us/step - loss: 0.4677\n",
      "Epoch 91/200\n",
      "78292/78292 [==============================] - 53s 672us/step - loss: 0.4676\n",
      "Epoch 92/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78292/78292 [==============================] - 53s 675us/step - loss: 0.4675\n",
      "Epoch 93/200\n",
      "78292/78292 [==============================] - 53s 676us/step - loss: 0.4674\n",
      "Epoch 94/200\n",
      "78292/78292 [==============================] - 53s 671us/step - loss: 0.4675\n",
      "Epoch 95/200\n",
      "78292/78292 [==============================] - 53s 675us/step - loss: 0.4674\n",
      "Epoch 96/200\n",
      "78292/78292 [==============================] - 53s 675us/step - loss: 0.4675\n",
      "Epoch 97/200\n",
      "78292/78292 [==============================] - 53s 677us/step - loss: 0.4674\n",
      "Epoch 98/200\n",
      "78292/78292 [==============================] - 53s 672us/step - loss: 0.4671\n",
      "Epoch 99/200\n",
      "78292/78292 [==============================] - 53s 678us/step - loss: 0.4671\n",
      "Epoch 100/200\n",
      "78292/78292 [==============================] - 53s 671us/step - loss: 0.4671\n",
      "Epoch 101/200\n",
      "78292/78292 [==============================] - 53s 675us/step - loss: 0.4670\n",
      "Epoch 102/200\n",
      "78292/78292 [==============================] - 53s 673us/step - loss: 0.4669\n",
      "Epoch 103/200\n",
      "78292/78292 [==============================] - 53s 675us/step - loss: 0.4670\n",
      "Epoch 104/200\n",
      "78292/78292 [==============================] - 53s 674us/step - loss: 0.4669\n",
      "Epoch 105/200\n",
      "78292/78292 [==============================] - 52s 668us/step - loss: 0.4670\n",
      "Epoch 106/200\n",
      "78292/78292 [==============================] - 53s 674us/step - loss: 0.4667\n",
      "Epoch 107/200\n",
      "78292/78292 [==============================] - 52s 668us/step - loss: 0.4664\n",
      "Epoch 108/200\n",
      "78292/78292 [==============================] - 53s 682us/step - loss: 0.4665\n",
      "Epoch 109/200\n",
      "78292/78292 [==============================] - 52s 669us/step - loss: 0.4668\n",
      "Epoch 110/200\n",
      "78292/78292 [==============================] - 53s 677us/step - loss: 0.4665\n",
      "Epoch 111/200\n",
      "78292/78292 [==============================] - 53s 674us/step - loss: 0.4665\n",
      "Epoch 112/200\n",
      "78292/78292 [==============================] - 53s 678us/step - loss: 0.4662\n",
      "Epoch 113/200\n",
      "78292/78292 [==============================] - 53s 678us/step - loss: 0.4664\n",
      "Epoch 114/200\n",
      "78292/78292 [==============================] - 53s 672us/step - loss: 0.4662\n",
      "Epoch 115/200\n",
      "78292/78292 [==============================] - 53s 678us/step - loss: 0.4663\n",
      "Epoch 116/200\n",
      "78292/78292 [==============================] - 53s 672us/step - loss: 0.4663\n",
      "Epoch 117/200\n",
      "78292/78292 [==============================] - 53s 676us/step - loss: 0.4663\n",
      "Epoch 118/200\n",
      "78292/78292 [==============================] - 52s 666us/step - loss: 0.4663\n",
      "Epoch 119/200\n",
      "78292/78292 [==============================] - 53s 677us/step - loss: 0.4663\n",
      "Epoch 120/200\n",
      "78292/78292 [==============================] - 53s 678us/step - loss: 0.4662\n",
      "Epoch 121/200\n",
      "78292/78292 [==============================] - 53s 672us/step - loss: 0.4660\n",
      "Epoch 122/200\n",
      "78292/78292 [==============================] - 51s 657us/step - loss: 0.4660\n",
      "Epoch 123/200\n",
      "78292/78292 [==============================] - 51s 656us/step - loss: 0.4661\n",
      "Epoch 124/200\n",
      "78292/78292 [==============================] - 52s 660us/step - loss: 0.4659\n",
      "Epoch 125/200\n",
      "78292/78292 [==============================] - 51s 653us/step - loss: 0.4660\n",
      "Epoch 126/200\n",
      "78292/78292 [==============================] - 52s 663us/step - loss: 0.4661\n",
      "Epoch 127/200\n",
      "78292/78292 [==============================] - 51s 651us/step - loss: 0.4659\n",
      "Epoch 128/200\n",
      "78292/78292 [==============================] - 52s 663us/step - loss: 0.4657\n",
      "Epoch 129/200\n",
      "78292/78292 [==============================] - 52s 662us/step - loss: 0.4658\n",
      "Epoch 130/200\n",
      "78292/78292 [==============================] - 52s 659us/step - loss: 0.4658\n",
      "Epoch 131/200\n",
      "78292/78292 [==============================] - 52s 667us/step - loss: 0.4657\n",
      "Epoch 132/200\n",
      "78292/78292 [==============================] - 52s 662us/step - loss: 0.4655\n",
      "Epoch 133/200\n",
      "78292/78292 [==============================] - 52s 662us/step - loss: 0.4657\n",
      "Epoch 134/200\n",
      "78292/78292 [==============================] - 52s 659us/step - loss: 0.4657\n",
      "Epoch 135/200\n",
      "78292/78292 [==============================] - 52s 670us/step - loss: 0.4654\n",
      "Epoch 136/200\n",
      "78292/78292 [==============================] - 52s 662us/step - loss: 0.4655\n",
      "Epoch 137/200\n",
      "78292/78292 [==============================] - 53s 672us/step - loss: 0.4655\n",
      "Epoch 138/200\n",
      "78292/78292 [==============================] - 53s 671us/step - loss: 0.4653\n",
      "Epoch 139/200\n",
      "78292/78292 [==============================] - 52s 663us/step - loss: 0.4654\n",
      "Epoch 140/200\n",
      "78292/78292 [==============================] - 53s 673us/step - loss: 0.4656\n",
      "Epoch 141/200\n",
      "78292/78292 [==============================] - 52s 663us/step - loss: 0.4654\n",
      "Epoch 142/200\n",
      "78292/78292 [==============================] - 53s 679us/step - loss: 0.4652\n",
      "Epoch 143/200\n",
      "78292/78292 [==============================] - 52s 670us/step - loss: 0.4652\n",
      "Epoch 144/200\n",
      "78292/78292 [==============================] - 53s 672us/step - loss: 0.4653\n",
      "Epoch 145/200\n",
      "78292/78292 [==============================] - 53s 673us/step - loss: 0.4653\n",
      "Epoch 146/200\n",
      "78292/78292 [==============================] - 53s 672us/step - loss: 0.4653\n",
      "Epoch 147/200\n",
      "78292/78292 [==============================] - 771s 10ms/step - loss: 0.4654\n",
      "Epoch 148/200\n",
      "78292/78292 [==============================] - 2562s 33ms/step - loss: 0.4650\n",
      "Epoch 149/200\n",
      "78292/78292 [==============================] - 547s 7ms/step - loss: 0.4651\n",
      "Epoch 150/200\n",
      "78292/78292 [==============================] - 501s 6ms/step - loss: 0.4649\n",
      "Epoch 151/200\n",
      "78292/78292 [==============================] - 52s 662us/step - loss: 0.4650\n",
      "Epoch 152/200\n",
      "78292/78292 [==============================] - 51s 652us/step - loss: 0.4651\n",
      "Epoch 153/200\n",
      "78292/78292 [==============================] - 51s 649us/step - loss: 0.4650\n",
      "Epoch 154/200\n",
      "78292/78292 [==============================] - 51s 657us/step - loss: 0.4651\n",
      "Epoch 155/200\n",
      "78292/78292 [==============================] - 57s 727us/step - loss: 0.4649\n",
      "Epoch 156/200\n",
      "78292/78292 [==============================] - 67s 852us/step - loss: 0.4650\n",
      "Epoch 157/200\n",
      "78292/78292 [==============================] - 62s 786us/step - loss: 0.4648\n",
      "Epoch 158/200\n",
      "78292/78292 [==============================] - 67s 858us/step - loss: 0.4648\n",
      "Epoch 159/200\n",
      "78292/78292 [==============================] - 61s 777us/step - loss: 0.4650\n",
      "Epoch 160/200\n",
      "78292/78292 [==============================] - 60s 769us/step - loss: 0.4648\n",
      "Epoch 161/200\n",
      "78292/78292 [==============================] - 60s 761us/step - loss: 0.4650\n",
      "Epoch 162/200\n",
      "78292/78292 [==============================] - 60s 768us/step - loss: 0.4648\n",
      "Epoch 163/200\n",
      "78292/78292 [==============================] - 58s 746us/step - loss: 0.4648\n",
      "Epoch 164/200\n",
      "78292/78292 [==============================] - 52s 667us/step - loss: 0.4648\n",
      "Epoch 165/200\n",
      "78292/78292 [==============================] - 54s 696us/step - loss: 0.4649\n",
      "Epoch 166/200\n",
      "78292/78292 [==============================] - 72s 919us/step - loss: 0.4649\n",
      "Epoch 167/200\n",
      "78292/78292 [==============================] - 68s 874us/step - loss: 0.4649\n",
      "Epoch 168/200\n",
      "78292/78292 [==============================] - 71s 905us/step - loss: 0.4645\n",
      "Epoch 169/200\n",
      "78292/78292 [==============================] - 69s 878us/step - loss: 0.4645\n",
      "Epoch 170/200\n",
      "78292/78292 [==============================] - 127s 2ms/step - loss: 0.4644\n",
      "Epoch 171/200\n",
      "78292/78292 [==============================] - 134s 2ms/step - loss: 0.4646\n",
      "Epoch 172/200\n",
      "78292/78292 [==============================] - 136s 2ms/step - loss: 0.4645\n",
      "Epoch 173/200\n",
      "78292/78292 [==============================] - 118s 2ms/step - loss: 0.4645\n",
      "Epoch 174/200\n",
      "78292/78292 [==============================] - 132s 2ms/step - loss: 0.4646\n",
      "Epoch 175/200\n",
      "78292/78292 [==============================] - 131s 2ms/step - loss: 0.4648 0s - loss: 0\n",
      "Epoch 176/200\n",
      "78292/78292 [==============================] - 135s 2ms/step - loss: 0.4644\n",
      "Epoch 177/200\n",
      "78292/78292 [==============================] - 134s 2ms/step - loss: 0.4645\n",
      "Epoch 178/200\n",
      "78292/78292 [==============================] - 135s 2ms/step - loss: 0.4644\n",
      "Epoch 179/200\n",
      "78292/78292 [==============================] - 133s 2ms/step - loss: 0.4643\n",
      "Epoch 180/200\n",
      "78292/78292 [==============================] - 133s 2ms/step - loss: 0.4643\n",
      "Epoch 181/200\n",
      "78292/78292 [==============================] - 124s 2ms/step - loss: 0.4644\n",
      "Epoch 182/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78292/78292 [==============================] - 137s 2ms/step - loss: 0.4644\n",
      "Epoch 183/200\n",
      "78292/78292 [==============================] - 145s 2ms/step - loss: 0.4645\n",
      "Epoch 184/200\n",
      "78292/78292 [==============================] - 139s 2ms/step - loss: 0.4644\n",
      "Epoch 185/200\n",
      "78292/78292 [==============================] - 136s 2ms/step - loss: 0.4643ETA: 2s - l\n",
      "Epoch 186/200\n",
      "78292/78292 [==============================] - 132s 2ms/step - loss: 0.4641\n",
      "Epoch 187/200\n",
      "78292/78292 [==============================] - 83s 1ms/step - loss: 0.4643\n",
      "Epoch 188/200\n",
      "78292/78292 [==============================] - 78s 994us/step - loss: 0.4643\n",
      "Epoch 189/200\n",
      "78292/78292 [==============================] - 69s 876us/step - loss: 0.4643\n",
      "Epoch 190/200\n",
      "78292/78292 [==============================] - 94s 1ms/step - loss: 0.4643\n",
      "Epoch 191/200\n",
      "78292/78292 [==============================] - 68s 864us/step - loss: 0.4642\n",
      "Epoch 192/200\n",
      "78292/78292 [==============================] - 78s 991us/step - loss: 0.4641\n",
      "Epoch 193/200\n",
      "78292/78292 [==============================] - 74s 943us/step - loss: 0.4642\n",
      "Epoch 194/200\n",
      "78292/78292 [==============================] - 62s 798us/step - loss: 0.4642\n",
      "Epoch 195/200\n",
      "78292/78292 [==============================] - 60s 761us/step - loss: 0.4641\n",
      "Epoch 196/200\n",
      "78292/78292 [==============================] - 60s 767us/step - loss: 0.4640\n",
      "Epoch 197/200\n",
      "78292/78292 [==============================] - 60s 762us/step - loss: 0.4639\n",
      "Epoch 198/200\n",
      "78292/78292 [==============================] - 60s 770us/step - loss: 0.4642\n",
      "Epoch 199/200\n",
      "78292/78292 [==============================] - 58s 738us/step - loss: 0.4640\n",
      "Epoch 200/200\n",
      "78292/78292 [==============================] - 52s 664us/step - loss: 0.4642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1acfdb5c0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "epochs = 20\n",
    "model, enc = DAE(inp_names)\n",
    "\n",
    "model.fit(input_data, output, batch_size=BATCH_SIZE, epochs= 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78292, 3000)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = enc.predict(input_data)\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denseNN():\n",
    "    inp_layers = []\n",
    "    for i in range(3000):\n",
    "        inp_layers.append(Input([1]))\n",
    "    \n",
    "    x = concatenate(inp_layers)\n",
    "    x = Dense(4500, kernel_initializer='glorot_normal')(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    x = Dense(4500, kernel_initializer='glorot_normal')(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inp_layers, out)\n",
    "    optim = SGD(lr=0.0003, decay=0.995, momentum=0.99, clipnorm = 1)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = optim)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = denseNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54808, 14)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('../input/train_LZdllcl.csv')\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_data = emb[:54808, :]\n",
    "test_data = emb[54808:, :]\n",
    "inp_data_list = []\n",
    "test_data_list = []\n",
    "for i in range(3000):\n",
    "    inp_data_list.append(inp_data[:, i])\n",
    "    test_data_list.append(test_data[:, i])\n",
    "    \n",
    "y = np.array(df_train['is_promoted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "cvlist = KFold(n_splits=5, random_state=11).split(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 43846 samples, validate on 10962 samples\n",
      "Epoch 1/50\n",
      "43846/43846 [==============================] - 223s 5ms/step - loss: 0.3352 - val_loss: 0.3041\n",
      "Epoch 2/50\n",
      "43846/43846 [==============================] - 243s 6ms/step - loss: 0.2942 - val_loss: 0.3017\n",
      "Epoch 3/50\n",
      "43846/43846 [==============================] - 228s 5ms/step - loss: 0.2921 - val_loss: 0.3014\n",
      "Epoch 4/50\n",
      "43846/43846 [==============================] - 224s 5ms/step - loss: 0.2915 - val_loss: 0.3013\n",
      "Epoch 5/50\n",
      "43846/43846 [==============================] - 223s 5ms/step - loss: 0.2910 - val_loss: 0.3012\n",
      "Epoch 6/50\n",
      "22656/43846 [==============>...............] - ETA: 1:46 - loss: 0.2915"
     ]
    }
   ],
   "source": [
    "for run in range(3):\n",
    "    for tr_idx, val_idx in cvlist:\n",
    "        oof_preds = np.zeros(inp_data.shape[0])\n",
    "        preds = []\n",
    "        tr_data, val_data = inp_data[tr_idx, :], inp_data[val_idx, :]\n",
    "        y_tr, y_val = y[tr_idx], y[val_idx]\n",
    "        tr_data_list = []\n",
    "        val_data_list = []\n",
    "        for i in range(3000):\n",
    "            tr_data_list.append(tr_data[:, i])\n",
    "            val_data_list.append(val_data[:, i])\n",
    "\n",
    "        model.fit(tr_data_list, y_tr, batch_size=128, epochs= 50, validation_data= (val_data_list, y_val))\n",
    "        oof_preds[val_idx] = model.predict(val_data_list)\n",
    "        preds.append(model.predict(test_data_list))\n",
    "        np.mean(preds, axis = 0)\n",
    "    np.save(f'../oof_preds_{run}.npy', oof_preds)\n",
    "    np.save(f'../test_preds_{run}.npy', preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
